Learning rate is 0.01

Optimizer is Adadelta
Epoch & Fidelity & Runtime 
1 0.043695 23.998532
2 0.049803 47.888611
3 0.053469 71.795889
4 0.055807 95.798988
5 0.057418 119.797431
6 0.058545 143.729065
7 0.059405 167.893467
8 0.06023 191.864316
9 0.060911 215.965791
10 0.061568 240.076176
11 0.06218 264.077211
12 0.062819 288.257303
13 0.063445 312.452726

Optimizer is Adam
Epoch & Fidelity & Runtime 
1 0.292052 23.612233
2 0.405177 47.949729
3 0.41442 72.387566
4 0.486512 96.468449
5 0.519659 120.724469
6 0.595334 144.810035
7 0.605033 169.071451
8 0.61166 193.297741
9 0.620136 217.352725
10 0.639876 241.961842
11 0.518455 266.123963
12 0.415012 290.818416
13 0.291351 315.250699

Optimizer is Adamax
Epoch & Fidelity & Runtime 
1 0.191421 24.089722
2 0.282349 48.291989
3 0.357898 72.791718
4 0.399541 97.622511
5 0.427243 121.923053
6 0.45621 146.438628
7 0.4736 170.690823
8 0.504354 195.413653
9 0.523408 219.916782
10 0.509453 244.441397
11 0.546775 269.007423
12 0.550149 293.554514
13 0.568911 317.834862

Optimizer is SGD
Epoch & Fidelity & Runtime 
1 0.090584 23.586976
2 0.147557 47.748826
3 0.18847 72.23018
4 0.217975 96.73221
5 0.23991 121.213649
6 0.259926 145.323987
7 0.277934 170.020459
8 0.292955 194.104488
9 0.309406 218.817837
10 0.321915 243.287653
11 0.336523 267.408145
12 0.349591 291.856567
13 0.360462 316.296328

Optimizer is SGD $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.339966 24.375698
2 0.428101 48.043269
3 0.450297 72.57847
4 0.506779 96.820201
5 0.547805 121.369171
6 0.586102 145.871663
7 0.606808 170.527764
8 0.621208 195.031642
9 0.633387 219.552986
10 0.658405 243.74654
11 0.685511 268.287344
12 0.708901 292.527489
13 0.692185 317.489641

Optimizer is NAG $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.31469 24.311835
2 0.404 48.727134
3 0.462505 73.061221
4 0.51101 97.779844
5 0.545674 122.1611
6 0.577042 147.285655
7 0.610477 172.011122
8 0.599876 196.318588
9 0.610281 220.971841
10 0.655464 245.668214
11 0.690163 269.991342
12 0.682812 294.748119
13 0.679407 319.534962

