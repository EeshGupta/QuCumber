Learning rate is 0.01

Optimizer is Adadelta
Epoch & Fidelity & Runtime 
1 0.047578 24.499352
2 0.050778 49.434633
3 0.05297 76.484782
4 0.054549 100.939822
5 0.055738 125.691203
6 0.056717 150.732928
7 0.057547 175.441446
8 0.058283 200.375139
9 0.058931 225.409064
10 0.059615 250.36796
11 0.060237 275.270921
12 0.060872 300.221687

Optimizer is Adam
Epoch & Fidelity & Runtime 
1 0.313996 24.467094
2 0.390473 49.548795
3 0.431737 74.847191
4 0.47817 99.704688
5 0.517301 124.825469
6 0.575838 150.460567
7 0.523254 176.10755
8 0.538374 201.015496
9 0.585718 226.235981
10 0.661286 251.194563
11 0.529654 275.999484
12 0.515026 301.16828

Optimizer is Adamax
Epoch & Fidelity & Runtime 
1 0.208095 24.81508
2 0.302337 49.506283
3 0.361561 74.432409
4 0.410475 99.395459
5 0.45426 124.096946
6 0.48851 149.319371
7 0.519045 174.334148
8 0.533532 199.145945
9 0.554324 224.194285
10 0.57029 249.270144
11 0.596788 274.120418
12 0.590814 299.183633
13 0.59744 324.224902

Optimizer is SGD
Epoch & Fidelity & Runtime 
1 0.093711 24.511889
2 0.146243 49.275135
3 0.184844 74.102989
4 0.216395 99.229758
5 0.243281 123.839143
6 0.268715 148.703708
7 0.286885 173.614496
8 0.305358 198.283899
9 0.320682 223.239946
10 0.335494 248.487502
11 0.349207 273.235113
12 0.364485 298.333394
13 0.373579 323.424856

Optimizer is SGD $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.289671 24.283036
2 0.368896 49.578071
3 0.43418 74.610297
4 0.460484 99.368733
5 0.513986 124.41699
6 0.561258 149.429022
7 0.566549 174.247268
8 0.590209 199.679096
9 0.586331 224.825826
10 0.669979 249.613931
11 0.661947 274.737909
12 0.669152 299.830137
13 0.63557 324.600618

Optimizer is NAG $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.321586 24.712129
2 0.42343 49.181253
3 0.458495 74.148428
4 0.510093 98.838094
5 0.57866 123.960051
6 0.586485 149.374097
7 0.657078 174.21219
8 0.662868 199.358792
9 0.667407 224.532971
10 0.63668 249.416112
11 0.728693 274.648057
12 0.660025 299.911235
13 0.745408 325.199102

