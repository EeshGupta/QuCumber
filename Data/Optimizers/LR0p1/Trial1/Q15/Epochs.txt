Learning rate is 0.1

Optimizer is Adadelta
Epoch & Fidelity & Runtime 
1 0.062577 23.821787
2 0.07207 47.579459
3 0.084307 71.357779
4 0.098269 95.242401
5 0.112124 119.101742
6 0.125828 142.877077
7 0.138026 166.774463
8 0.149315 190.558391
9 0.159366 214.63588
10 0.169717 238.598043
11 0.179478 262.426536
12 0.18814 286.408737
13 0.195523 310.240815

Optimizer is Adam
Epoch & Fidelity & Runtime 
1 0.32565 23.204332
2 0.427591 47.204657
3 0.399936 71.243159
4 0.220081 95.297208
5 0.207094 119.322237
6 0.113161 143.114345
7 0.069465 167.175396
8 0.046223 191.290257
9 0.080096 215.435143
10 0.056946 239.601462
11 0.065912 263.561913
12 0.101611 287.743766
13 0.021927 311.768054

Optimizer is Adamax
Epoch & Fidelity & Runtime 
1 0.463976 23.662677
2 0.391066 47.587381
3 0.52593 71.622586
4 0.489791 95.875425
5 0.619991 120.124842
6 0.453455 144.407334
7 0.297122 168.950008
8 0.321158 193.055673
9 0.130957 217.409228
10 0.270173 241.771218
11 0.448947 265.925898
12 0.45929 290.301505
13 0.366822 314.685937

Optimizer is SGD
Epoch & Fidelity & Runtime 
1 0.317916 23.29419
2 0.426108 47.391926
3 0.430767 71.567776
4 0.500486 95.52466
5 0.527564 119.72551
6 0.545365 143.901212
7 0.55386 168.12613
8 0.59161 192.340679
9 0.602261 216.325316
10 0.630766 240.597708
11 0.640629 264.627412
12 0.64612 289.190527
13 0.642434 313.499247

Optimizer is SGD $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.348366 24.073373
2 0.493714 47.81853
3 0.388344 72.063323
4 0.457293 96.347874
5 0.243965 120.633075
6 0.370918 144.952401
7 0.382961 169.037804
8 0.219079 193.37835
9 0.551094 217.471382
10 0.415442 241.87903
11 0.479626 266.59626
12 0.416728 290.764977
13 0.546161 315.174413

Optimizer is NAG $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.331505 23.698224
2 0.420697 47.749098
3 0.185142 72.503842
4 0.395558 96.98024
5 0.488293 121.11879
6 0.434025 145.865017
7 0.525966 170.378179
8 0.460506 194.636096
9 0.508667 219.235846
10 0.463867 243.469005
11 0.323806 268.157907
12 0.201298 293.132934
13 0.455734 317.871967

