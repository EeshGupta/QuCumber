Learning rate is 0.001

Optimizer is Adadelta
Epoch & Fidelity & Runtime 
1 0.035016 30.7803
2 0.036122 61.829207
3 0.037177 91.853821
4 0.038189 125.39058
5 0.039132 159.634746
6 0.040054 187.77639
7 0.040913 217.562176
8 0.041735 247.287466
9 0.042498 276.333263
10 0.043255 306.08969

Optimizer is Adam
Epoch & Fidelity & Runtime 
1 0.086553 27.567606
2 0.13789 55.798068
3 0.177936 85.181558
4 0.20874 116.839792
5 0.238817 147.10322
6 0.266376 178.693261
7 0.291225 211.447361
8 0.311972 246.405381
9 0.33107 278.321783
10 0.348189 312.197095

Optimizer is Adamax
Epoch & Fidelity & Runtime 
1 0.060868 31.437997
2 0.071807 59.282175
3 0.089819 89.499932
4 0.111037 120.229368
5 0.130052 152.773913
6 0.147732 183.307652
7 0.161894 214.882905
8 0.174194 246.525717
9 0.185495 279.301887
10 0.196385 308.50768

Optimizer is SGD
Epoch & Fidelity & Runtime 
1 0.055629 32.766874
2 0.058455 63.959229
3 0.060958 95.820251
4 0.063902 128.252082
5 0.06734 159.419988
6 0.071276 191.302746
7 0.075808 223.906724
8 0.080721 255.615718
9 0.086202 286.550416
10 0.091565 318.51943

Optimizer is SGD $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.086134 31.966048
2 0.1354 61.776135
3 0.176944 92.587982
4 0.206887 122.175682
5 0.232753 155.395447
6 0.253609 188.212319
7 0.275429 218.843652
8 0.29482 250.882175
9 0.310428 283.329476
10 0.323877 314.798584

Optimizer is NAG $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.096788 28.683167
2 0.148069 58.157045
3 0.184599 86.831125
4 0.216021 117.197153
5 0.240157 149.403527
6 0.262498 187.217297
7 0.278781 220.888652
8 0.296582 254.580306
9 0.311455 289.774503
10 0.325121 323.039376

