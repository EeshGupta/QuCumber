Learning rate is 0.001

Optimizer is Adadelta
Epoch & Fidelity & Runtime 
1 0.045415 23.789912
2 0.046096 47.521957
3 0.046739 71.245991
4 0.047337 95.010885
5 0.047896 118.806606
6 0.04843 142.529838
7 0.048934 166.348269
8 0.049403 190.093136
9 0.049849 214.00142
10 0.050268 237.775905
11 0.050664 261.654468
12 0.051042 285.549705
13 0.051396 309.525253

Optimizer is Adam
Epoch & Fidelity & Runtime 
1 0.090847 22.728587
2 0.141502 46.312719
3 0.180587 70.129082
4 0.214334 94.071423
5 0.239857 117.865291
6 0.261879 141.876353
7 0.279654 165.790956
8 0.298421 189.566141
9 0.312743 213.439245
10 0.329203 237.268346
11 0.347809 261.193417
12 0.361799 285.040609
13 0.381295 309.130836

Optimizer is Adamax
Epoch & Fidelity & Runtime 
1 0.064291 23.254965
2 0.076923 47.334848
3 0.097364 71.54774
4 0.118568 95.737749
5 0.139064 119.938189
6 0.154638 144.132881
7 0.169672 168.396782
8 0.183408 192.468518
9 0.194938 216.714834
10 0.206385 240.854827
11 0.216991 265.280484
12 0.228083 289.906611
13 0.237632 314.124436

Optimizer is SGD
Epoch & Fidelity & Runtime 
1 0.054383 23.696069
2 0.05842 47.732449
3 0.061305 71.845322
4 0.064395 96.181668
5 0.068097 120.811449
6 0.072539 144.876735
7 0.077713 169.124473
8 0.0834 193.138369
9 0.089464 217.413515
10 0.096139 241.99929
11 0.102731 266.094278
12 0.10891 290.537442
13 0.11487 314.626785

Optimizer is SGD $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.085408 23.497353
2 0.145862 47.885941
3 0.187463 72.625036
4 0.214682 96.849065
5 0.236292 121.287022
6 0.255425 145.466089
7 0.273033 169.904492
8 0.28954 194.347988
9 0.304932 218.687076
10 0.320841 243.064995
11 0.334884 267.561289
12 0.34815 291.785592
13 0.362254 316.204386

Optimizer is NAG $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.100349 23.892669
2 0.151755 47.81902
3 0.188462 72.374155
4 0.219387 96.685605
5 0.246929 121.23679
6 0.270069 145.804612
7 0.293909 170.142639
8 0.31219 195.071474
9 0.32893 219.703339
10 0.346644 243.9266
11 0.362754 268.544667
12 0.371942 293.206909
13 0.386887 317.506354

