Learning rate is 0.01

Optimizer is Adadelta
Epoch & Fidelity & Runtime 
1 0.040256 25.913782
2 0.04597 51.937109
3 0.049488 77.682251
4 0.051957 103.686703
5 0.053791 129.48377
6 0.055206 155.066931
7 0.056351 179.33747
8 0.057341 203.231316
9 0.058229 227.158183
10 0.059032 250.928374
11 0.059766 274.901909
12 0.060521 298.945499
13 0.061229 322.792974

Optimizer is Adam
Epoch & Fidelity & Runtime 
1 0.325568 22.963222
2 0.447806 46.642472
3 0.505868 70.151776
4 0.578892 93.928753
5 0.593017 117.447884
6 0.564115 141.225765
7 0.587025 164.838501
8 0.55226 189.117808
9 0.527606 213.216387
10 0.589018 237.371768
11 0.590238 261.535934
12 0.515979 285.46323
13 0.491941 309.742341

Optimizer is Adamax
Epoch & Fidelity & Runtime 
1 0.205862 23.634796
2 0.30063 47.103316
3 0.356418 71.230549
4 0.398282 95.357267
5 0.43215 119.510081
6 0.4628 143.608747
7 0.491534 167.959016
8 0.508712 192.352941
9 0.516988 216.762101
10 0.533357 240.879514
11 0.56631 265.355258
12 0.557593 289.835231
13 0.573387 314.365776

Optimizer is SGD
Epoch & Fidelity & Runtime 
1 0.093038 23.18806
2 0.144157 47.190544
3 0.178773 70.894675
4 0.208886 95.073921
5 0.234726 118.778552
6 0.256495 143.268994
7 0.280104 167.474831
8 0.298602 191.394077
9 0.317319 215.804945
10 0.33279 240.251016
11 0.349085 264.304524
12 0.364511 289.228707
13 0.371413 313.263643

Optimizer is SGD $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.302515 23.426542
2 0.386217 47.146406
3 0.438781 70.937398
4 0.434653 95.195645
5 0.466181 119.936954
6 0.528979 143.761172
7 0.578835 168.147873
8 0.604356 192.718574
9 0.631156 216.808228
10 0.635575 241.390913
11 0.654781 265.97056
12 0.688465 290.594214
13 0.713975 315.193362

Optimizer is NAG $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.295712 24.030236
2 0.372461 47.354478
3 0.424397 71.759825
4 0.461008 96.212273
5 0.477221 120.649587
6 0.540688 145.086166
7 0.597518 169.169518
8 0.578331 193.871645
9 0.597746 219.083413
10 0.633882 243.244184
11 0.637784 267.924896
12 0.682078 292.248592
13 0.692628 317.109671

