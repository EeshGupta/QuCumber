Learning rate is 0.001

Optimizer is Adadelta
Epoch & Fidelity & Runtime 
1 0.035574 24.517998
2 0.036468 49.02637
3 0.03732 73.443466
4 0.038143 97.814241
5 0.038941 122.38262
6 0.039698 146.788989
7 0.040405 171.321131
8 0.041097 195.903553
9 0.041762 220.370655
10 0.042386 245.268839
11 0.042986 270.046012
12 0.043554 294.629925
13 0.044112 319.475552

Optimizer is Adam
Epoch & Fidelity & Runtime 
1 0.086006 24.000105
2 0.143845 48.446382
3 0.19087 73.060197
4 0.228941 97.846278
5 0.264161 122.26647
6 0.296479 146.845161
7 0.317712 171.642841
8 0.342182 196.243453
9 0.359462 221.331872
10 0.377648 246.215167
11 0.389212 270.876071
12 0.4016 295.81034
13 0.410642 320.807463

Optimizer is Adamax
Epoch & Fidelity & Runtime 
1 0.061718 23.61653
2 0.075357 48.462192
3 0.094908 73.403639
4 0.114757 98.053475
5 0.130374 123.065934
6 0.144059 148.083764
7 0.155646 172.864633
8 0.166886 198.045861
9 0.176154 223.262051
10 0.186485 248.185227
11 0.196252 273.753303
12 0.205979 299.02986
13 0.21678 324.024667

Optimizer is SGD
Epoch & Fidelity & Runtime 
1 0.055025 24.178862
2 0.058334 48.812603
3 0.060623 73.722014
4 0.063286 98.259709
5 0.066363 123.150939
6 0.069931 148.311197
7 0.073901 172.958619
8 0.07847 198.004183
9 0.08313 223.099445
10 0.087891 247.885445
11 0.092902 273.050691
12 0.097617 298.544827
13 0.102489 323.35267

Optimizer is SGD $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.104351 24.498079
2 0.15358 48.822522
3 0.18733 73.431342
4 0.214942 98.443326
5 0.23901 123.394811
6 0.260853 148.028636
7 0.281978 173.135938
8 0.301891 198.740472
9 0.318621 223.664287
10 0.331605 249.587286
11 0.344922 274.940985
12 0.357887 299.913483
13 0.368504 325.372207

Optimizer is NAG $\gamma$ = 0.9
Epoch & Fidelity & Runtime 
1 0.096466 24.444299
2 0.150025 49.871975
3 0.182028 74.661672
4 0.210926 100.00056
5 0.240938 125.289397
6 0.272683 150.113492
7 0.298516 175.53131
8 0.323487 200.971503
9 0.343306 226.85385
10 0.36244 251.830899
11 0.376168 277.244615
12 0.393374 302.854851

